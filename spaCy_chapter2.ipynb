{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spaCy_chapter2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diem-ai/natural-language-processing/blob/master/spaCy_chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x8lV9p0hcrV",
        "colab_type": "text"
      },
      "source": [
        "Chapter 2: Large-scale data analysis with spaCy\n",
        "\n",
        "In this chapter, you'll use your new skills to extract specific information from large volumes of text. You''ll learn how to make the most of spaCy's data structures, and how to effectively combine statistical and rule-based approaches for text analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hhty0E4h_ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "# Import the Doc class\n",
        "from spacy.tokens import Doc, Span\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from spacy.matcher import Matcher, PhraseMatcher"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaQtURv2iCLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7AuMNV2hjSB",
        "colab_type": "text"
      },
      "source": [
        "**String to hashesh**\n",
        "\n",
        "- Look up the string “cat” in nlp.vocab.strings to get the hash.\n",
        "- Look up the hash to get back the string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7pPcYXBhUSq",
        "colab_type": "code",
        "outputId": "8654464d-ee58-42cd-ed20-9fb13fff3104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "doc = nlp(\"I have a cat\")\n",
        "\n",
        "# Look up the hash for the word \"cat\"\n",
        "cat_hash = nlp.vocab.strings[\"cat\"]\n",
        "print(cat_hash)\n",
        "\n",
        "# Look up the cat_hash to get the string\n",
        "cat_string = nlp.vocab.strings[cat_hash]\n",
        "print(cat_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5439657043933447811\n",
            "cat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFzRMhLEiclz",
        "colab_type": "text"
      },
      "source": [
        "- Look up the string label “PERSON” in nlp.vocab.strings to get the hash.\n",
        "- Look up the hash to get back the string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsIVjwknib0M",
        "colab_type": "code",
        "outputId": "0b338b83-306f-4ac0-ce77-e47b55b0dac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "doc = nlp(\"David Bowie is a PERSON\")\n",
        "\n",
        "# Look up the hash for the string label \"PERSON\"\n",
        "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
        "print(person_hash)\n",
        "\n",
        "# Look up the person_hash to get the string\n",
        "person_string = nlp.vocab.strings[person_hash]\n",
        "print(person_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "380\n",
            "PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVBDzUT0j5w0",
        "colab_type": "text"
      },
      "source": [
        "**Creating a Doc from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgur_LWoj_59",
        "colab_type": "code",
        "outputId": "cfbfd85e-671c-4ee8-bd4b-72b5806d5e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "\n",
        "\n",
        "# Desired text: \"spaCy is cool!\"\n",
        "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
        "spaces = [True, True, False, False]\n",
        "\n",
        "# Create a Doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spaCy is cool!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2yD-CV5leFw",
        "colab_type": "text"
      },
      "source": [
        "Create a sentence: \"Go, get started!\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC35xIDvk4BS",
        "colab_type": "code",
        "outputId": "d7a9b9c0-e9f6-4d58-8e74-cd60871f1782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Desired text: \"Go, get started!\"\n",
        "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
        "spaces = [False, True, True, False, False]\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go, get started!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQqjNOoPmW0g",
        "colab_type": "text"
      },
      "source": [
        "Create a sentence: \"Oh, really?!\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8kuJFOymXRP",
        "colab_type": "code",
        "outputId": "5b04b961-1e3f-4d77-cf92-ca1bea8ae2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Desired text: \"Oh, really?!\"\n",
        "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
        "spaces = [False, True, False, False, False]\n",
        "\n",
        "# Create a Doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words = words, spaces = spaces)\n",
        "print(doc.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oh, really?!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZKDktm_nG1j",
        "colab_type": "text"
      },
      "source": [
        "**Doc, Span, Entities from Scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYuU0bXJvAi3",
        "colab_type": "text"
      },
      "source": [
        "- In this exercise, you’ll create the Doc and Span objects manually, and update the named entities – just like spaCy does behind the scenes. A shared nlp object has already been created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3iqwmXknPhT",
        "colab_type": "code",
        "outputId": "bee079a6-62ff-4497-a46b-aabf09ff48e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "nlp_en = English()\n",
        "\n",
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
        "spaces = [True, True, True, False]\n",
        "\n",
        "# Create a doc from the words and spaces\n",
        "doc = Doc(nlp_en.vocab, words=words, spaces=spaces)\n",
        "print(doc.text)\n",
        "\n",
        "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
        "span = Span(doc, 2, 4, label=\"PERSON\")\n",
        "print(span.text, span.label_)\n",
        "\n",
        "# Add the span to the doc's entities\n",
        "doc.ents = [span]\n",
        "\n",
        "# Print entities' text and labels\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I like David Bowie\n",
            "David Bowie PERSON\n",
            "[('David Bowie', 'PERSON')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH52R3m9u9cQ",
        "colab_type": "text"
      },
      "source": [
        "**Data Structure Best Practices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAwekINtvEdY",
        "colab_type": "code",
        "outputId": "86c538ea-a374-4c8e-db70-f094757dd324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Berlin is a nice capital. Munich is historical city. We should stay there for a while. \")\n",
        "\n",
        "# Get all tokens and part-of-speech tags\n",
        "token_texts = [token.text for token in doc]\n",
        "pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "for index, pos in enumerate(pos_tags):\n",
        "    # Check if the current token is a proper noun\n",
        "    if pos == \"PROPN\":\n",
        "        # Check if the next token is a verb\n",
        "        if pos_tags[index + 1] == \"VERB\":\n",
        "            result = token_texts[index]\n",
        "            print(\"Found proper noun before a verb:\", result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found proper noun before a verb: Berlin\n",
            "Found proper noun before a verb: Munich\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9rfhKCYvQ7A",
        "colab_type": "code",
        "outputId": "c90b7e26-de2d-4f37-a054-c86323936fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(pos_tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PROPN', 'VERB', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'PROPN', 'VERB', 'ADJ', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', 'PUNCT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYOXtG4VwZfs",
        "colab_type": "code",
        "outputId": "acfeba6e-0e08-45eb-8b2f-b860ff280493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Berlin is a nice capital. Munich is historical city. We should stay there for a while. \")\n",
        "\n",
        "for token in doc:\n",
        "  if(token.pos_ == \"PROPN\"):\n",
        "    if( (token.i + 1) < len(doc) and (doc[token.i + 1].pos_ == \"VERB\") ):\n",
        "      print(\"Found proper noun before a verb:\", token.text)\n",
        "# print(token.tag_, token.pos_, token.ent_type_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found proper noun before a verb: Berlin\n",
            "Found proper noun before a verb: Munich\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W_19lCv1jcC",
        "colab_type": "text"
      },
      "source": [
        "**Inspect Word Vector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjNcNr3kyzKX",
        "colab_type": "code",
        "outputId": "163fdefe-2890-4727-bd38-30976b70d3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "# Process a text\n",
        "doc = nlp(\"Two bananas in pyjamas\")\n",
        "\n",
        "# Get the vector for the token \"bananas\"\n",
        "bananas_vector = doc[1].vector\n",
        "print(bananas_vector)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.1561384   0.6859281  -1.8234854   0.4145496  -0.886605    5.0773377\n",
            "  0.28650832  3.6156225  -2.627604    5.01052     2.6055033   5.4986916\n",
            " -0.82726336 -2.4128723  -1.5714562   0.67344356 -1.1230624   3.017315\n",
            "  3.4531426   2.6312394  -2.3144596   2.0717711  -0.5736556  -0.5199362\n",
            " -0.4892068   1.4417053   1.1748309   3.291245    2.7368522   2.1909308\n",
            "  2.4100504  -1.5442916  -0.81270695 -1.7967525  -2.4401696   0.96489155\n",
            " -5.071314    2.4865592  -1.1760099   1.0010973  -1.8218107   6.159581\n",
            "  5.876448   -1.9877293   6.579393    1.0499439  -1.5798447  -4.1203165\n",
            " -0.17076118 -4.819325   -2.1152763  -4.640588    1.5844907  -3.2757292\n",
            "  2.1921952  -0.47692332 -1.8678508   1.0092752   0.7716696  -0.37776387\n",
            "  0.07058215 -0.18511617  5.209738   -3.002555   -1.8404679   4.089005\n",
            " -2.0230193   1.0394226  -1.7199193   1.0383378   0.23976706 -0.67239416\n",
            "  1.3192352  -0.33726573  0.21724188 -0.5032941   0.26279616 -0.58214176\n",
            " -3.0981517  -4.9684753  -3.2268834  -4.5933228  -3.0618596  -0.02340478\n",
            "  2.299171   -3.6006322  -1.8415104  -0.14858985  1.3507223  -1.3084451\n",
            " -5.810022   -0.10575765  0.84036636  3.035629   -0.48342353 -0.67074174]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UtQIqj21RWz",
        "colab_type": "code",
        "outputId": "1bda7b17-802c-45fd-b1eb-213313558933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(len(bananas_vector))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLtGypfr1qTX",
        "colab_type": "text"
      },
      "source": [
        "**Compare Similarity**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q56vj8y_2PrE",
        "colab_type": "text"
      },
      "source": [
        "Use the doc.similarity method to compare doc1 to doc2 and print the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlahkv1n3KDv",
        "colab_type": "code",
        "outputId": "8775546b-accd-42dd-9d1c-bb6431816b2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOxl3nXX3YgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_md = spacy.load(\"en_core_web_md\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NMiEdNr1tsW",
        "colab_type": "code",
        "outputId": "91dd1fbf-10a6-47b5-bdb8-5a94e44e6eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "doc1 = nlp_md(\"It's a warm summer day\")\n",
        "doc2 = nlp_md(\"It's sunny outside\")\n",
        "\n",
        "# Get the similarity of doc1 and doc2\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8789265574516525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGyTzQ264NlV",
        "colab_type": "text"
      },
      "source": [
        "Use the token.similarity method to compare token1 to token2 and print the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MBvjeu14Hmq",
        "colab_type": "code",
        "outputId": "478a5134-c229-45d9-f375-d9dfaa1fb7e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "doc = nlp_md(\"TV and books\")\n",
        "token1, token2 = doc[0], doc[2]\n",
        "\n",
        "# Get the similarity of the tokens \"TV\" and \"books\"\n",
        "similarity = token1.similarity(token2)\n",
        "print(similarity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22325331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-vJoz7n2Q8n",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYr8GN8P5lXL",
        "colab_type": "code",
        "outputId": "1d3572f0-384a-4436-c2e1-ea4551445086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "doc = nlp_md(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
        "\n",
        "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
        "span1 = Span(doc, 3, 5)\n",
        "span2 = Span(doc, 12, 15)\n",
        "\n",
        "print(span1)\n",
        "print(span2)\n",
        "# Get the similarity of the spans\n",
        "similarity = span1.similarity(span2)\n",
        "print(similarity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "great restaurant\n",
            "really nice bar\n",
            "0.75173926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSsTGa5z5o7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iskb9cvkAw5j",
        "colab_type": "code",
        "outputId": "e36038f1-1725-477c-c2fb-c1307854f4da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "doc = nlp_md(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
        "\n",
        "pattern1 = [{\"TEXT\": \"great\"}, {\"TEXT\": \"restaurant\"}]\n",
        "\n",
        "matcher = Matcher(nlp_md.vocab)\n",
        "matcher.add(\"PATTERN1\", None, pattern1)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Print pattern string name and text of matched span\n",
        "    print(doc.vocab.strings[match_id], doc[start:end].text)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PATTERN1 great restaurant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ5vLXeq_szn",
        "colab_type": "text"
      },
      "source": [
        "**Combining models and rules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecS5xQRUFDbi",
        "colab_type": "text"
      },
      "source": [
        "- Both patterns in this exercise contain mistakes and won’t match as expected. Can you fix them? If you get stuck, try printing the tokens in the doc to see how the text will be split and adjust the pattern so that each dictionary represents one token.\n",
        "\n",
        "- Edit pattern1 so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
        "- Edit pattern2 so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKBpDQFP_vU5",
        "colab_type": "code",
        "outputId": "330709c9-3bc6-46db-ca2c-f186d393da58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "doc = nlp(\n",
        "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
        "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
        "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
        "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
        "    \"Prime for new members, beginning on September 14. However, members with \"\n",
        "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
        "    \"viewing until their subscription comes up for renewal. Those with \"\n",
        "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
        ")\n",
        "\n",
        "# Create the match patterns\n",
        "pattern1 = [{\"LOWER\": \"amazon\"}, {\"POS\": \"PROPN\"}]\n",
        "#pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
        "pattern2 = [{\"LOWER\": \"ad\"}, {\"POS\": \"PUNCT\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
        "\n",
        "# Initialize the Matcher and add the patterns\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"PATTERN1\", None, pattern1)\n",
        "matcher.add(\"PATTERN2\", None, pattern2)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Print pattern string name and text of matched span\n",
        "    print(doc.vocab.strings[match_id], start, end, doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PATTERN1 7 9 Amazon Prime\n",
            "PATTERN2 27 31 ad-free viewing\n",
            "PATTERN1 39 41 Amazon Prime\n",
            "PATTERN2 44 48 ad-free viewing\n",
            "PATTERN2 82 86 ad-free viewing\n",
            "PATTERN2 102 106 ad-free viewing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "116mpcoEFnuD",
        "colab_type": "text"
      },
      "source": [
        "**Efficient PharseMatcher**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDQOMwGZF1ih",
        "colab_type": "text"
      },
      "source": [
        "- Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world. We already have a list of countries, so let’s use this as the basis of our information extraction script. A list of string names is available as the variable COUNTRIES.\n",
        "\n",
        "- Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n",
        "- Add the phrase patterns and call the matcher on the doc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKyAcxZeCU3_",
        "colab_type": "code",
        "outputId": "efe48f0d-9cf2-477c-bfc1-52fb8843dc03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "COUNTRIES = ['Afghanistan', 'Åland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia (Plurinational State of)', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'United States Minor Outlying Islands', 'Virgin Islands (British)', 'Virgin Islands (U.S.)', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cabo Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo (Democratic Republic of the)', 'Cook Islands', 'Costa Rica', 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', \"Côte d'Ivoire\", 'Iran (Islamic Republic of)', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia (the former Yugoslav Republic of)', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia (Federated States of)', 'Moldova (Republic of)', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \"Korea (Democratic People's Republic of)\", 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine, State of', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of Kosovo', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Korea (Republic of)', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom of Great Britain and Northern Ireland', 'United States of America', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela (Bolivarian Republic of)', 'Viet Nam', 'Wallis and Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']\n",
        "\n",
        "doc = nlp_en(\"Czech Republic may help Slovakia protect its airspace. France is an ally of United States of America\")\n",
        "\n",
        "matcher = PhraseMatcher(nlp_en.vocab)\n",
        "# Create pattern Doc objects and add them to the matcher\n",
        "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
        "patterns = list(nlp_en.pipe(COUNTRIES))\n",
        "matcher.add(\"COUNTRY\", None, *patterns)\n",
        "\n",
        "# Call the matcher on the test document and print the result\n",
        "matches = matcher(doc)\n",
        "\n",
        "print(matches)\n",
        "\n",
        "print([doc[start:end] for match_id, start, end in matches])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(4000319556510314152, 0, 2), (4000319556510314152, 4, 5), (4000319556510314152, 9, 10), (4000319556510314152, 14, 18)]\n",
            "[Czech Republic, Slovakia, France, United States of America]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fA7jlVKKMXV",
        "colab_type": "text"
      },
      "source": [
        "**Extract Countries and Relationship**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5YYwWLuKTeS",
        "colab_type": "text"
      },
      "source": [
        "- In the previous exercise, you wrote a script using spaCy’s PhraseMatcher to find country names in text. Let’s use that country matcher on a longer text, analyze the syntax and update the document’s entities with the matched countries.\n",
        "\n",
        "- Iterate over the matches and create a Span with the label \"GPE\" (geopolitical entity).\n",
        "- Overwrite the entities in doc.ents and add the matched span.\n",
        "- Get the matched span’s root head token.\n",
        "- Print the text of the head token and the span."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "231mKvAmJ_U5",
        "colab_type": "code",
        "outputId": "7df345b8-a4f6-4773-bc24-8e853b31d04e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "matcher = PhraseMatcher(nlp_en.vocab)\n",
        "patterns = list(nlp_en.pipe(COUNTRIES))\n",
        "matcher.add(\"COUNTRY\", None, *patterns)\n",
        "\n",
        "# Create a doc and find matches in it\n",
        "doc = nlp_en(\"Czech Republic may help Slovakia protect its airspace. France is an ally of United States of America\")\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Create a Span with the label for \"GPE\"\n",
        "    span = Span(doc, start, end, label=\"GPE\")\n",
        "\t\n",
        "    #print(doc.ents)\n",
        "    # Overwrite the doc.ents and add the span\n",
        "    doc.ents = list(doc.ents) + [span]\n",
        "    \n",
        "     # Get the span's root head token\n",
        "    span_root_head = span.root.head\n",
        "    # Print the text of the span root's head token and the span text\n",
        "    print(span_root_head.text, \"-->\", span.text)\n",
        "\n",
        "# Print the entities in the document\n",
        "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Czech --> Czech Republic\n",
            "Slovakia --> Slovakia\n",
            "France --> France\n",
            "United --> United States of America\n",
            "[('Czech Republic', 'GPE'), ('Slovakia', 'GPE'), ('France', 'GPE'), ('United States of America', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}